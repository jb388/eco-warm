---
title: "swaedie_test"
author: "J. Beem-Miller"
date: "2024-03-08"
output: html_document
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      fig.align = 'center', dev = c('cairo_pdf', 'png'))
options(scipen = 5)
# load read ess_dive fx
source("./utilities/read_ess-dive.fx.R")
```

```{r setup, include = FALSE}
library(ggplot2)
library(tidyr)
library(SoilR)
library(openxlsx)
library(ISRaD)
library(lme4)
library(lmerTest)
library(emmeans)
library(gt)
library(scales)
library(mpspline2)
library(dplyr)
library(lubridate)
library(terra)
library(tidyterra)
```

```{r utils}
# add carriage return to .csv files (recursive by default)
crAdd <- function(dataDir, ..) {
  ls <- list.files(path = dataDir, pattern = ".csv$", recursive = TRUE, full.names = TRUE)
  sapply(ls, function(file) write.table(
    "", file = file, sep = ",", append = TRUE, quote = FALSE, col.names = FALSE, row.names = FALSE))
}

# print and append message to "outfile" if verbose == TRUE
vcat <- function(..., append = TRUE) if (verbose) cat(..., file = outfile, append = append)

# read core data
readCore <- function(coreDir, return = "template") {
  # get dir name as needed
  if (missing(coreDir)) {
    coreDir <- "../data/sweddie/metadata/core"
  }
  # create template list
  ls <- list.files(coreDir, full.names = TRUE)
  ix.dd <- grep("_dd", ls)
  ls.dd <- ls[ix.dd]
  ls.tm <- ls[-ix.dd]
  dd <- lapply(
    setNames(ls.dd, nm = sub(pattern = "(.*)\\..*$", replacement = "\\1", basename(ls.dd))),
    read.csv)
  if (return == "template") {
    lapply(
      setNames(dd, nm = c("experiment", "plot", "site")), function(x) {
        setNames(data.frame(matrix(ncol = nrow(x), nrow = 0)), x[ , 1])
      })
  } else {
    return(dd)
  }
}

# checks column names
checkColNms <- function(metadata, tableName, datIn, err, ...) {
  
  # check for template
  if (missing(metadata)) metadata <- readMeta()
  
  # check tableNames
  if (!names(datIn)[which(names(datIn) == tableName)] %in% names(metadata)[which(names(metadata) == tableName)])  {
    err <- err + 1
    vcat(warning(
      paste("\t", names(datIn)[tableName], "is missing in data file\n")))
  }
  
  # check for missing cols in data
  miss <- setdiff(colnames(metadata[[tableName]]), colnames(datIn[[tableName]]))
  
  # check for extra cols in data
  xtra <- setdiff(colnames(datIn[[tableName]]), colnames(metadata[[tableName]]))

  if (length(miss) > 0 | length(xtra) > 0) {
    err <- err + 1
    vcat(warning(
      paste("\t", names(datIn[tableName]), "table names do not match template\n")))
    if (length(miss) > 0) vcat("\t\tColumn names missing:", miss, "\n")
    if (length(xtra > 0)) vcat("\t\tColumn names extra:", xtra, "\n")
  }
  return(err)
}

siteMap <- function(database) {
  # stopifnot(is_israd_database(database))

  latlon <- database$site[, 3:4]
  world <- map_data("world")
  ggplot() +
    geom_polygon(data = world, aes(x = .data$long, y = .data$lat, group = .data$group), fill = NA, color = "Black") +
    geom_point(data = latlon, aes(x = .data$sit_long, y = .data$sit_lat), color = "red", size = 2, alpha = 0.5) +
    theme_bw(base_size = 16) +
    labs(title = "SWEDDIE: Site_Map", x = "Longitude", y = "Latitude")
}
```

# Build core database

```{r sweddie-assemble}
# function for assembling database
DIR = "../data/sweddie"
write_report = TRUE
write_out = TRUE
output_ext = c(".csv", ".rda")
verbose = TRUE

# Constants
DB_DIR <- "database"
S_DIR <- "siteData"
LIST_FILE <- "coreData.rda"
TIMESTAMP <- format(Sys.time(), "%y%m%d-%H%M") 
  
# Set output file
outfile <- ""
if (write_report) {
  outfile <- file.path(DIR, DB_DIR, paste0("logs/coreLog", "_", TIMESTAMP, ".txt"))
  file.create(outfile)
}

# Start writing in the output file
vcat("SWEDDIE Compilation Log \n",
     "\n", as.character(Sys.time()),
     "\n", rep("-", 15), "\n")


# Get the tables stored in the template sheets
metadata <- readCore()

vcat("\n\nCompiling data files in", S_DIR, "\n", rep("-", 30), "\n")

data_dirs <- list.dirs(file.path(DIR, S_DIR), full.names = TRUE, recursive = FALSE)
if (!length(data_dirs)) {
  vcat(warning("No data directories found!\n"))
  return(NULL)
}

# ensure EOL carriage return present
crAdd(file.path(DIR, S_DIR))

vcat("Compiling and checking site data...\n")
  #   pb <- txtProgressBar(min = 0, max = length(data_dirs), style = 3)
  # }
  # 
  # check if previous database object exists in database directory, and only update file if new data exisit
if (file.exists(file.path(DIR, DB_DIR, LIST_FILE))) {
  
  # load existing database
  load(file.path(dataset_directory, DB_DIR, LIST_FILE)) # obj "coreDat"
  
  # convert to character and coerce to list of data frames
  coreDat_chr <- lapplydf(lapply(coreData, function(x) lapply(x, as.character)))

  # remove old version of ISRaD
  rm(coreDat)

  # Split each table by entry_name
  coreDat_old <- lapply(coreDat_chr, function(x) split(x, x$exp_name))
} else {
  database <- metadata
}

# compile new templates and check against existing data
for (d in seq_along(data_dirs)) {
  
  # get site name
  site_nm <- basename(data_dirs[d])
  
  vcat(paste(site_nm, "\n"))
  
  # get tables
  tbls <- list.files(data_dirs[d], full.names = TRUE)
  
  # check that required files are present
  ix <- which(!names(metadata) %in% sub("\\..*", "", basename(tbls)))
  if (length(ix) > 0) {
    vcat(warning(
      paste0(paste(names(metadata)[ix], collapse = ", "), 
             " files not found for site ", site_nm, "!\n")))
  } 
  
  # read files
  datIn <- lapply(setNames(tbls, nm = sub("\\..*", "", basename(tbls))), read.csv)
  
  # check data fidelity against template
  err <- 0
  for (i in seq_along(datIn)) {
    
    # check column names
    err <- checkColNms(metadata, names(datIn)[i], datIn, err)
    
    # check data types
    
    # check data values
    
    # bind to database
    if (err == 0) database[[i]] <- rbind(database[[i]], datIn[[i]]) 
  }
}
```

# read_meta function
Reads data dictionary and file level metadata files

```{r read-dat}
read_meta <- function (expName) {
  
  # get site dir paths and variable directory names
  exp.ls <- list.dirs("../data/experiments", recursive = FALSE)
  exp.dir <- exp.ls[match(expName, basename(exp.ls))]
  dat.dir <- list.files(file.path(exp.dir, "data"), full.names = TRUE)
  
  # check files
  
  flmd.ls <- dat.dir[which(grepl("_flmd.csv", dat.dir))]
  dd.ls <- dat.dir[which(grepl("_dd.csv", dat.dir))]
  dat.ls <- gsub("_dd", "", dd.ls)
  
  if (sum(length(flmd.ls), length(dd.ls), length(dat.ls)) != length(dat.dir)) {
    warning (paste("Non-standard files present in ", expName, " data directory. Only data files and accompanying flmd and dd files allowed.", "\n"))
  }
  
  if (any(is.na(match(dat.ls, dat.dir)))) {
    warning (paste(expName, " data and data dictionary file names do not match", "\n"))
  }

  # read flmd files
  flmd <- lapply(flmd.ls, read.csv)
  names(flmd) <- gsub("\\.csv", "", basename(flmd.ls))
  
  # check flmd file names against data files
  if (any(is.na(unlist(lapply(flmd, function(x) {
    unlist(
      sapply(
        glob2rx(x$fileName), grep, basename(dat.ls), USE.NAMES = F))
  }))))) {
    warning (paste(expName, " data file names do not match flmd names", "\n"))
    }
  
  # read dd files
  dd <- lapply(dd.ls, read.csv)
  names(dd) <- gsub("\\.csv", "", basename(dd.ls))
  
  return(
    list(flmd = flmd, dd = dd)
  )
}

# old code
  # # extract file level metadata (flmd) & data dictionary (dd) files
  # unlist(Filter(Negate(is.null), lapply(seq_along(dat.dir), function(i) {
  #     
  #     # check for flmd files
  #   if (any(grepl("_flmd.csv", list.files(dat.dir[i])))) {
  #     flmd.ls <- lapply(as.list(list.files(dat.dir[i], pattern = "_flmd.csv", full.names = TRUE)), read.csv)
  #           
  #     # get dat names and ensure data files are present
  #     dat.nms <- unlist(lapply(flmd.ls, "[[", "fileName"))
  #     if (length(match(dat.nms, list.files(dat.dir[i]))) != length(dat.nms)) {
  #       warning("files named in file level metadata not found")
  #     }
  #     
  #     # define names for flmd & dd lists
  #     flmd.nms <- paste0(
  #       sapply(strsplit(dat.nms, "\\."), "[[", 1), "_flmd")
  #     dd.nms <- paste0(
  #       sapply(strsplit(dat.nms, "\\."), "[[", 1), "_dd")
  #     
  #     # extract dd files
  #     dd.ls <- lapply(seq_along(dd.nms), function(j)
  #       read.csv(file.path(var.dirs[i], paste0(dd.nms[j], ".csv"))))
  #     
  #     # assign names for flmd & dd lists
  #     names(flmd.ls) <- flmd.nms
  #     names(dd.ls) <- dd.nms
  #     
  #     list(flmd = flmd.ls, dd = dd.ls)
  #   }
  #   })
  #   ), recursive = FALSE)
```

```{r helper-fxs}
# template for creating data dictionary files
dd_helper <- function(expName, dataName, DATA_DIR = NULL, write_out = TRUE) {
  if (is.null(DATA_DIR)) {
    DATA_DIR <- file.path("~/eco-warm/data/experiments", expName, "data") 
  }
  template <- read.csv("~/eco-warm/data/sweddie/metadata/datTemplate_dd.csv")
  data <- read.csv(file.path(DATA_DIR, paste0(dataName, ".csv")))
  template[1:ncol(data), 1] <- names(data)
  if (write_out) {
    write.csv(x = template, 
              file = file.path(DATA_DIR, paste0(dataName, "_dd.csv")),
              row.names = FALSE)
  } else {
    template
  }
}

# template for flmd
flmd_helper <- function(expName, dataFileName, dateColName, rename = FALSE, append = TRUE, write_out = TRUE, ...) {

  # set data directory path
  DATA_DIR <- file.path("~/eco-warm/data/experiments", expName, "data") 
  
  # get flmd template
  if (append) {
    files <- list.files(
        DATA_DIR,
        full.names = TRUE)
    flmd.s <- as.list(files[grepl("flmd", files)])
    if (length(flmd.s) == 0) {
      stop ("cannot append record: no flmd files found")
    }
    if (length(flmd.s) > 1) {
     i <- menu(
      basename(flmd.s), 
      title = "To which FLMD should new record be appended?") 
    } else {
      i <- 1
    }
    flmdName <- flmd.s[[i]]
    flmd <- read.csv(flmdName)
  } else {
    flmd <- read.csv("~/eco-warm/data/sweddie/metadata/flmd_SWEDDIE.csv")
  }
  
  # get data
  data <- read.csv(file.path(DATA_DIR, paste0(dataFileName, ".csv")))
  
  # fill out flmd
  nm <- vector(length = ncol(flmd), mode = "list")
  names(nm) <- names(flmd)
  if (Jeff) {
    nm[["Name"]] <- "Jeffrey Beem-Miller"
    nm[["email"]] <- "jbeemmiller@lbl.gov"
  }
  nm[["expName"]] <- expName
  nm[["fileName"]] <- paste0(dataFileName, ".csv")
  if (!is.null(orders)) {
    data[[dateColName]] <- parse_date_time(data[[dateColName]], orders = orders)
  }
  nm[["startDate"]] <- as.character(min(data[[dateColName]], na.rm = TRUE))
  nm[["endDate"]] <- as.character(max(data[[dateColName]], na.rm = TRUE))
  
  # helper function for menus
  menu.fx <- function(opt = unique(flmd[[i]])) {
    if (is.null(nm[[i]])) {
      j <- menu(c("yes", "no"), title = paste0("Do you want to use a previously entered ", names(nm)[i], "?")) 
      if (j == 1) {
        sel <- menu(opt, "Please use one of the following options for your data (enter '0' if none are appropriate)")
        if (sel != 0) {
          opt[sel]
        } else {
          nm[[i]] <- readline(prompt = paste0(names(flmd)[i], "? ")) 
        } 
      } else {
        nm[[i]] <- readline(prompt = paste0(names(flmd)[i], "? ")) 
      } 
    }
  }
  
  for (i in seq_along(nm)) {
    if (is.null(nm[[i]])) {
      if (names(nm)[i] == "Name") {
        nm[[i]] <- menu.fx()
      } else if (names(nm)[i] == "email") {
        nm[[i]] <- menu.fx()
      } else if (names(nm)[i] == "dataContactName") {
        nm[[i]] <- menu.fx()
      } else if (names(nm)[i] == "dataContactEmail") {
        nm[[i]] <- menu.fx()
      } else if (names(nm)[i] == "UTC_offset") {
        nm[[i]] <- menu.fx()
      } else if (names(nm)[i] == "dataType") {
        if (!is.null(flmd_dd.ls)) {
          dataType_opts <- unique(unlist(lapply(lapply(flmd_dd.ls, "[[", "flmd"), function(x) lapply(x, "[[", "dataType"))))
          nm[[i]] <- menu.fx(opt = dataType_opts)
        }
      } else {
        nm[[i]] <- readline(prompt = paste0(names(flmd)[i], "? ")) 
      }
    }
  }
  
  nr <- nrow(flmd)
  if (nr == 0) {
    flmd[1, ] <- unlist(nm)
  } else {
    flmd[nr + 1, ] <- unlist(nm)
  }
  if (write_out) {
    if (rename) {
      write.csv(x = flmd, file = file.path(DATA_DIR, paste0(expName, "_flmd.csv")), row.names = FALSE) 
    } else {
      if (is.null(flmdName)) {
        flmdName <- file.path(DATA_DIR, paste0(expName, "_flmd.csv"))
      }
      write.csv(x = flmd, file = flmdName, row.names = FALSE) 
    }
  } else {
    flmd
  }
}
```

# Visualize core data

```{r plots}
# get KG data
tifRaster <- terra::rast("/Users/jeff/Seafile/ISRaD_geospatial_data/Beck_KG_V1/Beck_KG_V1_present_0p5.tif")
terra::crs(tifRaster) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
ext <- terra::extract(tifRaster, cbind(database$site$sit_long, database$site$sit_lat))
key.df <- data.frame(
  utils::read.csv("/Users/jeff/Seafile/ISRaD_geospatial_data/ISRaD_extra_keys/KG_x_x_x_x_present.csv",
                  stringsAsFactors = FALSE))
database$site <- cbind(database$site, key.df[ext[ , 1], c(2,3)])

newcols <- coltab(tifRaster)
newcols[[1]][["alpha"]] <- 100
lg <- newcols[[1]][2:31, ]
lg <- cbind(lg, key.df[, 2:3])

kg <- tifRaster
coltab(kg) <- newcols
lvls <- rbind(key.df[ , c(1, 2)], data.frame("ID" = seq(31, 255, 1), "pro_KG_present_short" = 0))
lg.ch <- rgb(lg[,2:4], maxColorValue = 255)
names(lg.ch) <- lg$pro_KG_present_short
levels(kg) <- lvls

# heat levels
exp_hlvls <- separate_longer_delim(database$experiment, "heat_levels", ";")
exp_hlvls$heat_levels <- as.numeric(exp_hlvls$heat_levels)
plot_hlvl.df <- exp_hlvls[exp_hlvls$heat_levels != 0, ]
hist(plot_hlvl.df$heat_levels, breaks = length(unique(exp_hlvls$heat_levels)))

# heat mtd BG
exp_hmtdB <- separate_longer_delim(database$experiment, "heat_mtd_belowGround", ";")

# MAT & MAP
exp.site <- database$experiment %>%
  left_join(database$site, by = c("exp_name")) %>%
  mutate(MAP.scaled = MAP / 10)

ggplot(exp.site, aes(MAT, MAP, color = pro_KG_present_short)) +
  geom_point(size = 4) +
  scale_color_manual(name = "Köppen-Geiger Zone", 
                     breaks = names(lg.ch), 
                     values = lg.ch) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        legend.position = "none")

# get stand along legend for KG
lg.ch.long <- lg.ch
names(lg.ch.long) <- lg$pro_KG_present_long
kg.legend <- cowplot::get_legend(
  exp.site %>%
    mutate(pro_KG_present_long = factor(pro_KG_present_long, levels = names(lg.ch.long))) %>%
    ggplot(., aes(MAT, MAP, color = pro_KG_present_long)) +
    geom_point(size = 4) +
    scale_color_manual(name = "Köppen-Geiger Zone", 
                       breaks = names(lg.ch.long), 
                       values = lg.ch.long, 
                       drop = FALSE) +
    theme_bw() +
    theme(panel.grid = element_blank())
)
grid::grid.newpage()
grid::grid.draw(kg.legend)

# MAT v. MAP w/ PM
ggplot(exp.site, aes(MAT, MAP, color = parent_material)) +
  geom_point(size = 4) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        legend.position = "none") 

# stand along legend for PM
pm.legend <- cowplot::get_legend(
 ggplot(exp.site, aes(MAT, MAP, color = parent_material)) +
  geom_point(size = 4) +
  theme_bw() +
  theme(panel.grid = element_blank()) 
)
grid::grid.newpage()
grid::grid.draw(pm.legend)

# plot land use
left_join(database$experiment, database$site, by = "exp_name", multiple = "first") %>%
  mutate(biome = ifelse(grepl("tundra", biome), "tundra", 
                        ifelse(grepl("peat", biome), "peatland",
                               ifelse(grepl("grassland", biome), "grassland", biome))),
         biome = reorder(biome, biome, function(x) -length(x))) %>%
  ggplot(., aes(biome, fill = biome)) +
  geom_bar() +
  scale_fill_manual(
    values = c("temperate forest" = "#97B669",
               "grassland" = "#FCD57A",
               "tundra" = "#C1E1DD",
               "tropical forest" = "#317A22",
               "boreal forest" = "#A5C790",
               "peatland" = "#D16E3F",
               "salt marsh" = "#db8d68",
               "subtropical forest" = "#75A95E")) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1)) 

# Map
latlon <- database$site[, 3:4]
world <- map_data("world")
ggplot() +
  geom_polygon(data = world, aes(x = .data$long, y = .data$lat, group = .data$group), fill = NA, color = "Black") +
  geom_point(data = latlon, aes(x = .data$sit_long, y = .data$sit_lat), color = "red", size = 3, alpha = 0.5) +
  theme_bw(base_size = 16) +
  labs(title = "SWEDDIE: Site_Map", x = "Longitude", y = "Latitude")

# parse dates
BG_i <- parse_date_time(exp.site$heat_BG_date_i, orders = c("%Y", "%m/%d/%y", "%m/%d/%Y", "%b-%y", "%d-%b"))
AG_i <-  parse_date_time(exp.site$heat_AG_date_i, orders = c("%Y", "%m/%d/%y", "%m/%d/%Y", "%b-%y", "%d-%b"))
dur.BG <- time_length(interval(BG_i, today()), "year")
dur.AG <- time_length(interval(AG_i, today()), "year")

exp.site$belowground <- dur.BG
exp.site$aboveground <- dur.AG

exp.site %>%
  pivot_longer(cols = c(belowground, aboveground), 
               names_to = "heating", 
               values_to = "duration") %>%
  arrange(desc(duration)) %>%
  mutate(exp_name = ordered(exp_name, levels = unique(exp_name))) %>%
  ggplot(., aes(duration, exp_name)) +
  geom_point(aes(shape = heating), size = 4) +
  scale_shape_manual(values = c("belowground" = 6, "aboveground" = 2)) +
  theme_bw() +
  theme(panel.grid = element_blank()) 
```

# Metadata

```{r read-meta-files}
exp_names <- list.files("../data/experiments")
flmd_dd.ls <- lapply(lapply(exp_names, read_meta), function(x) {
  if (length(x$flmd) == 0) {
    NULL
  } else {
    x
  }
})
names(flmd_dd.ls) <- exp_names
flmd_dd.ls <- Filter(Negate(is.null), flmd_dd.ls)
```

# Sites

Do we want to include MERIT & SMARTX? I guess they would serve as good end members for the analysis, i.e., demonstrating something about heating efficacy/variability as a function of soil moisture.

## [185ExperimentalStation]
- new candidate site in China
- wheat cropping experiment, north China Plain
- started in 2008
- AG infrared heating
- soil temp & moisture measurements to 40 cm
- PI Wenxu Dong

```{r 185ExpSt-data}
tempMoist185 <- read.csv("../data/experiments/185ExperimentStation/data/datafile.csv") %>%
  mutate(date = parse_date_time(date, orders = "%Y-%m-%d %H-%M"),
         soil.moisture = as.numeric(soil.moisture),
         soil.temperature = as.numeric(soil.temperature))
tempMoist185$plot.rep <- paste0(tempMoist185$PlotID.treatment., "_", tempMoist185$replicate)
```

## ACBB

## Achenkirch

## B4WarmED

## Blodgett
### ESS-DIVE data

```{r Blodgett-dat}
# read file
b_test <- read_ess.dive.fx("Blodgett")
```

### temp & moist data

```{r Blodgett-dat}
# read data files, save as uncompressed RDS for fast loading
moistBlg <- read.csv("/Users/jeff/eco-warm/data/raw/Blodgett/tempMoist/20240125_processed Blodgett soil moisture.csv", na.strings = "NAN")
saveRDS(moistBlg, "../data/raw/Blodgett/tempMoist/rds/20240125_processed Blodgett soil moisture.rds", compress = FALSE)
tempBlg <- read.csv("/Users/jeff/eco-warm/data/raw/Blodgett/tempMoist/20240125_processed Blodgett soil temperature.csv", na.strings = "NAN")
saveRDS(tempBlg, "../data/raw/Blodgett/tempMoist/rds/20240125_processed Blodgett soil temperature.rds", compress = FALSE)

# read in RDS
tempBlg <- readRDS("../data/raw/Blodgett/tempMoist/rds/20240125_processed Blodgett soil temperature.rds")
moistBlg <- readRDS("../data/raw/Blodgett/tempMoist/rds/20240125_processed Blodgett soil moisture.rds")
```

Notes on temp & moisture data
- soil temp & moisture measured at different depths
  - temp: 5, 15, 20, 30, 50, 70, 75, 100, 120, 140
  - moist: 10, 30, 50 90
- only 30, 50 in common
- aggregate to moisture depths for consistency

```{r T-M-blg-ter}
# aggregate data by date
tempBlg2 <- tempBlg
moistBlg2 <- moistBlg
tempBlg2$timestamp <- parse_date_time(tempBlg$TIMESTAMP, orders = "%Y-%m-%d %H-%M-%S")
moistBlg2$timestamp <- parse_date_time(moistBlg$TIMESTAMP, orders = "%Y-%m-%d %H-%M-%S")

# average by day
tempBlg2.d <- tempBlg2 %>% 
  mutate(date = date(timestamp),
         temp = as.numeric(temp)) %>%
  group_by(date, PP, depth, trt) %>% 
  summarize(mean.temp = mean(temp, na.rm = T), .groups = "drop") %>% 
  data.frame()

# spline to moisture depths
tempBlg2.d.sp <- bind_rows(lapply(split(tempBlg2.d, tempBlg2.d$date), function(x) {
  bind_rows(lapply(split(x, x$PP), function(y) {
    bind_rows(lapply(split(y, y$trt), function(z) {
      z <- z[order(z$depth), ]
      if (nrow(z) - length(which(is.na(z$mean.temp))) > 4) {
        sp <- spline(z$depth, z$mean.temp, method = "natural")
        ss <- smooth.spline(sp, lambda = .1, tol = 1e-4)
        sp.std <- predict(ss, c(10, 30, 50, 90))
        data.frame(depth = sp.std$x, mean.temp = sp.std$y)
      } else {
        NULL
      }
    }), .id = "trt")
  }), .id = "PP")
}), .id = "date")
tempBlg2.d.sp$date <- parse_date_time(tempBlg2.d.sp$date, orders = "%Y/%m/%d")

# average by day
moistBlg2.d <- moistBlg2 %>% 
  mutate(date = date(timestamp),
         moisture = as.numeric(moisture)) %>%
  group_by(date, PP, depth, trt) %>% 
  summarize(mean.moisture = mean(moisture, na.rm = T), .groups = "drop") %>% 
  data.frame()

# compare temp and moisture w/ depth
blg.t.m.d <- merge(
  tempBlg2.d.sp, 
  moistBlg2.d, 
  by = c("PP", "depth", "trt", "date"))
blg.t.m.d$dryWet <- ifelse(month(blg.t.m.d$date) > 10 | month(blg.t.m.d$date) < 4, "wet", "dry")
tm.blg.lm <- lm(mean.temp ~ mean.moisture, blg.t.m.d)
tmd.blg.lm <- lm(mean.temp ~ mean.moisture + depth, blg.t.m.d)
tmd2.blg.lm <- lm(mean.temp ~ mean.moisture * depth, blg.t.m.d)
tmdT.blg.lm <- lm(mean.temp ~ mean.moisture + depth + trt, blg.t.m.d)
tmdT2.blg.lm <- lm(mean.temp ~ mean.moisture * depth * trt, blg.t.m.d)
tmdT3.blg.lm <- lm(mean.temp ~ depth * trt * dryWet, blg.t.m.d)

# mixed model
m.m1 <- lmer(mean.moisture ~ depth * trt + (1 | PP), blg.t.m.d)

ggplot(blg.t.m.d, aes(mean.moisture, mean.temp, color = depth)) +
  geom_point() +
  facet_grid(cols = vars(trt)) +
  theme_bw() +
  theme(panel.grid = element_blank())

ggplot(blg.t.m.d, aes(mean.moisture, depth, color = trt)) +
  geom_jitter() +
  scale_color_manual(name = "", values = c("C" = "blue", "H" = "red")) +
  scale_y_reverse() +
  facet_grid(cols = vars(dryWet)) +
  theme_bw() +
  theme(panel.grid = element_blank())

blg.t.m.d %>% 
  group_by(trt, depth, dryWet) %>% 
  summarize(moist = mean(mean.moisture, na.rm = T), sd = sd(mean.moisture, na.rm = T), n = n()) %>%
  mutate(sem2 = 2 * (sd / sqrt(n))) %>%
  ggplot(., aes(depth, moist, fill = trt)) + 
  geom_col(position = "dodge") + 
  geom_errorbar(aes(ymin = moist - sem2, ymax = moist + sem2, color = trt), position = "dodge") +
  scale_fill_manual(values = c("H" = "#F8766D", "C" = "#00A9FF")) +
  scale_color_manual(values = c("H" = "#F8766D", "C" = "#00A9FF")) +
  facet_grid(cols = vars(dryWet)) + 
  theme_bw() + 
  theme(panel.grid = element_blank())

blg.t.m.d %>% 
  pivot_wider(id_cols = c(PP, depth, date, dryWet), names_from = trt, values_from = mean.moisture) %>%
  mutate(mDif = C - H) %>%
  group_by(depth, dryWet) %>% 
  summarize(mean.mDif = mean(mDif, na.rm = T), sd = sd(mDif, na.rm = T), n = n()) %>%
  mutate(sem2 = 2 * (sd / sqrt(n))) %>%
  ggplot(., aes(depth, mean.mDif, fill = dryWet)) + 
  geom_col(position = "dodge") + 
  geom_errorbar(
    aes(ymin = mean.mDif - sem2, ymax = mean.mDif + sem2, color = dryWet), position = "dodge") + 
  scale_fill_manual(values = c("dry" = "#CD9600", "wet" = "#00BE67")) +
  scale_color_manual(values = c("dry" = "#CD9600", "wet" = "#00BE67")) +
  theme_bw() + 
  theme(panel.grid = element_blank())
```

## CiPEHR
### temp & moist
```{r CiPEHR-dat}
# read data files, save as uncompressed RDS for fast loading
tmp_mstCpr <- read.csv("../data/experiments/CiPEHR/raw/flux_hh_v2.csv")
```

```{r convert-to-ts}
# explore ts
temp2 <- tmp_mstCpr
temp2$ts <- parse_date_time(temp2$ts, orders = "%Y-%m-%d %H-%M-%S")

# note that day light savings time causes conversion issue! e.g., 3/10/2019 2:00 through 3/10/2019 2:45 should not exist
ix <- which(is.na(temp2$TIMESTAMP))

# plot
dmean.temp.long <- pivot_longer(temp2, cols = 7:26, names_to = "sensor_ID", values_to = "temp")
dmean.temp.long <- left_join(
  dmean.temp.long,
  metadata[ , c("sensor_ID", "sensor_type", "sensor_depth")], 
  by = c("sensor_ID"))
dmean.temp.long %>%
  filter()
  ggplot(., aes())

temp2 <- temp2[-ix, ]
temp.ts <- xts(temp2, temp2$TIMESTAMP)

# calculate daily mean for different sensors
dmean.temp <- apply.daily(temp.ts[ , 7:26], mean, na.rm = TRUE)


# duplicates?
ix <- which(!duplicated(temp2$TIMESTAMP))
plot(temp2$TIMESTAMP[ix], temp2$Temp_T_Avg.1.[ix])

iix <- which(duplicated(temp2$TIMESTAMP)) # all NA...
t1 <- which(duplicated(temp2$Temp_T_Avg.1.[iix]))
```


## FORHOT

## Harvard Forest
- NB: no deep soil data!!


## KAEFS
```{r KAEFS-reshape}
kaefs.t <- read.csv("../data/experiments/KAEFS/derived/temperature_200908_201802_daily_v.csv")
kaefs.t.l <- pivot_longer(
  kaefs.t, 
  cols = 5:ncol(kaefs.t), 
  names_to = c("plotID", "blockID", "warmingTreatment", "precipTreatment", "clippingTreatment", "depth_cm"),
  names_sep = "_", 
  values_to = "T")
write.csv(kaefs.t.l, file = "../data/experiments/KAEFS/data/temperature_200908_201802_daily_reshaped.csv")

# ANPP
kaefs.anpp <- read.csv("../data/experiments/KAEFS/derived/ANPP_2009_2023_reshaped.csv")
kaefs.anpp.l <- pivot_longer(
  kaefs.anpp,
  cols = 5:ncol(kaefs.anpp),
  names_to = c("vegType", "year"),
  names_sep = "_",
  values_to = "g_m2_y"
)
write.csv(kaefs.anpp.l, file = "../data/experiments/KAEFS/data/ANPP_2009_2023_reshapedLong.csv", row.names = FALSE)
dd_helper("KAEFS", "ANPP_2009_2023_reshapedLong")

# soil moisture
kaefs.moist <- read.csv("../data/experiments/KAEFS/derived/SoilMoisture2018-2023_reshape.csv")
kaefs.moist.l <- pivot_longer(
  kaefs.moist,
  cols = 2:ncol(kaefs.moist),
  names_to = c("plotID", "depth"),
  names_sep = "_",
  values_to = "vwc_pct"
)
write.csv(kaefs.moist.l, file = "../data/experiments/KAEFS/data/SoilMoisture2018-2023_reshapeLong.csv", row.names = FALSE)
dd_helper("KAEFS", "SoilMoisture2018-2023_reshapeLong")

dd_helper("KAEFS", "local_climate_data")
flmd_helper(
  expName = "KAEFS", 
  dataFileName = "local_climate_data", 
  dateColName = "TIME",
  rename = TRUE, append = TRUE, write_out = TRUE,
  Jeff = TRUE,
  orders = "%Y-%m-%d %H:%M")


# check plot IDs
kaefs.t <- read.csv("../data/experiments/KAEFS/data/temperature_200908_201802_daily_reshaped.csv")
```

## Lyon, HI


## MERIT


## Sanming


## SMARTX


## SPRUCE


## SWAMP


## SWELTR


## TEAM

TEAM data provided as a sheet for each depth within an excel workbook. Code below reads data from there and outputs properly formatted .csv files. Note that original temperature excel workbook has summary data in rows below row number 1900; these are excluded during read in.

```{r TEAM-data-process}
dat.dir <- "../data/experiments/TEAM/data"
TEAM.temp.path <- "../data/experiments/TEAM/raw/soil temperature-TEAM 2018-2023.xlsx"
ix <- excel_sheets(TEAM.temp.path)[1:8]
TEAM.temp.ls <- lapply(seq_along(ix), function(i) {
  read_excel(TEAM.temp.path, sheet = i, n_max = 1900)
})
names(TEAM.temp.ls) <- ix
TEAM.temp.df <- bind_rows(TEAM.temp.ls, .id = "depth")
TEAM.temp.df.long <- pivot_longer(TEAM.temp.df, cols = starts_with("plot"), names_to = "plot", values_to = "temp") %>%
  mutate(plot = sub("plot", "", plot),
         depth = as.numeric(sub("cm", "", depth)))
write.csv(
  TEAM.temp.df.long, 
  file = file.path(dat.dir, "temp.csv"),
  row.names = FALSE)

TEAM.moist.path <- "../data/experiments/TEAM/raw/soil mositure-TEAM 2018-2023.xlsx"
ix <- excel_sheets(TEAM.moist.path)[1:6]
TEAM.moist.ls <- lapply(seq_along(ix), function(i) {
  read_excel(TEAM.moist.path, sheet = i)
})
names(TEAM.moist.ls) <- ix
TEAM.moist.df <- bind_rows(TEAM.moist.ls, .id = "depth")
TEAM.moist.df.long <- pivot_longer(TEAM.moist.df, cols = starts_with("plot"), names_to = "plot", values_to = "moist") %>%
  mutate(plot = sub("plot", "", plot),
         depth = as.numeric(sub("cm", "", depth)))
write.csv(
  TEAM.moist.df.long, 
  file = file.path(dat.dir, "moist.csv"),
  row.names = FALSE)
```

## TeRaCON
- four data streams
  1) "sentek": VWC, salinity, and temperature at 15 min intervals for roughly 5 years
  2) "temp": temp 
  3) "tdr": soil moisture from TDR sensors
  4) "Biocon_hourly_2012-2018vprimary1.csv" & ""

```{r TeRaCON-dat}
# # read data files, save as uncompressed RDS for fast loading
# sentek <- read.csv("../data/raw/TeRaCON/sentek2019_2023 data for distribution.csv")
# saveRDS(sentek, "../data/raw/TeRaCON/rds/sentek.rds", compress = FALSE)
# temp <- read.csv("../data/raw/TeRaCON/Temperature 2019-2023 data for distribution.csv", na.strings = "NAN")
# saveRDS(temp, "../data/raw/TeRaCON/rds/temp.rds", compress = FALSE)
# tdr <- read.csv("../data/raw/TeRaCON/TDR 2019-2023 data for distribution.csv")
# saveRDS(tdr, "../data/raw/TeRaCON/rds/tdr.rds", compress = FALSE)
sentek <- readRDS("../data/experiments/TeRaCON/rds/sentek.rds")
temp <- readRDS("../data/raw/TeRaCON/rds/temp.rds")
tdr <- readRDS("../data/raw/TeRaCON/rds/tdr.rds")

plt.info <- read.xlsx("../data/raw/TeRaCON/Plot table.xlsx", sheet = 1)
sns.info <- read.xlsx("../data/raw/TeRaCON/Plot table.xlsx", sheet = 4, cols = 1:5)
metadata <- read.csv("../data/raw/TeRaCON/metadata/metadata.csv")
```

```{r ter-temp-dat}
biocon.temp <- read.csv("../data/experiments/TeRaCON/Biocon_hourly_2012-2018vprimary1.csv")
biocon.temp.wmsn <- read.csv("../data/experiments/TeRaCON/Biocon_hourly_warmingseason_2012-2018vprimary1.csv")
```

```{r T-M-blg-ter}
# rename raw data
moistTempTer <- sentek

# Remove trailing period from col names
names(moistTempTer) <- sub('\\.$', '', names(moistTempTer))

# change names sep to be "_" instead of "."
names(moistTempTer) <- gsub(x = names(moistTempTer), pattern = "\\.", replacement = "_")

# drop salinity data and pivot longer
moistTempTer.long <- moistTempTer %>%
  select(c(TIMESTAMP, STATNAME, contains("VWC"), contains("TEMP"))) %>%
  mutate(across(-c(TIMESTAMP, STATNAME), as.numeric)) %>%
  pivot_longer(
    cols = !c(TIMESTAMP, STATNAME),
    names_to = c("var", "sensor", "depth"),
    names_sep = "_",
    values_to = "value") %>%
  mutate(date = date(parse_date_time(TIMESTAMP, orders = "%m/%d/%Y %H/%M"))) %>%
  select(-TIMESTAMP)

# add plot ID for the heat/control plots, and filter drought & eCO2 plots
moistTempTer.long$ring <- sub("Soil_", "", moistTempTer.long$STATNAME)
moistTempTer.long$plot <- ifelse(
 moistTempTer.long$ring == "2" & moistTempTer.long$sensor == "H", 106,
 ifelse(moistTempTer.long$ring == "2" & moistTempTer.long$sensor == "D", 119,
        ifelse(moistTempTer.long$ring == "4" & moistTempTer.long$sensor == "D", 226,
               ifelse(moistTempTer.long$ring == "4" & moistTempTer.long$sensor == "H", 233,
                      ifelse(moistTempTer.long$ring == "6" & moistTempTer.long$sensor == "C", 320, ifelse(moistTempTer.long$ring == "6" & moistTempTer.long$sensor == "H", 361, NA))))))

# remove NaN & NA
moistTempTer.ch <- na.omit(moistTempTer.long)

moistTer <- moistTempTer.ch %>%
  filter(var == "VWC") %>%
  select(var, date, plot, depth, value) %>%
  group_by(date, plot, depth) %>% 
  summarize(mean.moisture = mean(value, na.rm = T), .groups = "drop") %>%
  data.frame()
tempTer <- moistTempTer.ch %>%
  filter(var == "TEMP") %>%
  select(var, date, plot, depth, value) %>%
  group_by(date, plot, depth) %>% 
  summarize(mean.temp = mean(value, na.rm = T), .groups = "drop") %>% 
  data.frame()
moistTempTer.clean <- merge(
  moistTer,
  tempTer,
  by = c("plot", "depth", "date"))
moistTempTer.clean$trt <- ifelse(
  moistTempTer.clean$plot == 106 | moistTempTer.clean$plot == 233 | moistTempTer.clean$plot == 361, "C", "H")
    
# add seasonal info
moistTempTer.clean$season <- ifelse(
  month(moistTempTer.clean$date) < 4, "Q1", 
  ifelse(month(moistTempTer.clean$date) > 3 & month(moistTempTer.clean$date) < 7, "Q2",
         ifelse(month(moistTempTer.clean$date) > 6 & month(moistTempTer.clean$date) < 10, "Q3", "Q4")))
moistTempTer.clean$DEPTH <- as.numeric(moistTempTer.clean$depth) * 10

# models
ter.lm <- lm(mean.moisture ~ DEPTH * trt, moistTempTer.clean)

# plots
moistTempTer.clean %>%
  mutate(datePlot = paste0(date, plot)) %>%
  ggplot(., aes(mean.moisture, DEPTH, color = trt)) +
  geom_path(aes(group = datePlot)) +
  scale_color_manual(values = c("H" = "#F8766D", "C" = "#00A9FF")) +
  scale_y_reverse() +
  facet_grid(cols = vars(season)) +
  theme_bw() +
  theme(panel.grid = element_blank())

moistTempTer.clean %>% 
  group_by(trt, depth, season) %>% 
  summarize(moist = mean(mean.moisture, na.rm = T), sd = sd(mean.moisture, na.rm = T), n = n()) %>%
  mutate(sem2 = 2 * (sd / sqrt(n))) %>%
  ggplot(., aes(depth, moist, fill = trt)) + 
  geom_col(position = "dodge") + 
  geom_errorbar(aes(ymin = moist - sem2, ymax = moist + sem2, color = trt), position = "dodge") +
  scale_fill_manual(values = c("H" = "#F8766D", "C" = "#00A9FF")) +
  scale_color_manual(values = c("H" = "#F8766D", "C" = "#00A9FF")) +
  facet_grid(cols = vars(season)) + 
  theme_bw() + 
  theme(panel.grid = element_blank())

moistTempTer.clean %>% 
  mutate(PP = ifelse(plot < 200, 2, ifelse(plot > 200 & plot < 300, 4, 6))) %>%
  pivot_wider(id_cols = c(PP, DEPTH, date, season), names_from = trt, values_from = mean.moisture) %>%
  mutate(mDif = C - H) %>%
  group_by(DEPTH, season) %>% 
  summarize(mean.mDif = mean(mDif, na.rm = T), sd = sd(mDif, na.rm = T), n = n()) %>%
  mutate(sem2 = 2 * (sd / sqrt(n))) %>%
  ggplot(., aes(DEPTH, mean.mDif, fill = season)) + 
  geom_col(position = "dodge") + 
  geom_errorbar(
    aes(ymin = mean.mDif - sem2, ymax = mean.mDif + sem2, color = season), position = "dodge") + 
  theme_bw() + 
  theme(panel.grid = element_blank())
```


## TRACE


# General Workflow

How to assemble data files in an efficient manner? 
Do we need metadata from the files for efficient querying, which can then be used to retrieve the data?
How to do this?

For example, time intervals, depth intervals, data types. 

How do we organize/categorize sensor data?

Needs:
- standardized date/time format
- time series frequency detection tool? gap ID tool, etc.
- report time series

Need *annotation* tables
- these tables that are used for keyed translation
- header row consists of:
  1) "table_id"
  - describes contents of input data, e.g., "TeRaCON_temp_ts" (do we want naming conventions here?)
  2) "column_id"
  - names of columns in input data, e.g., `r names(temp)`
  3) "of_variable"
  - names of target variables in database, e.g., "temperature"
  4) "is_type"
  - decription of the type of data contained in column named by "column_id", e.g., `r names(temp)[1]` = "timestamp", `r names(temp)[2]` = "sensorID", `r names(temp)[7]` = "value"
  5) "with_entry"
  - flag for whether "column_id" values correspond to names in database, i.e., 
  - if("column_id" == "of_variable") do nothing else replace "column_id" with "of_variable" and "is_type"
  
Need to convert raw data into *shoestring* tuples
- these are an intermediate format for the data that is in "long" format to facilitate joins with the annotation table
- table names include: "table_id", "row_number", "column_id", "with_entry"
- workflow is as follows:
1) use `plyr::ldply` function to transform original data tables, giving each row a unique index
3) pivot the data longer, preserving row indices to allow grouping the data on the original rows
4) set the "table_id" column to the appropriate table name
5) join long data with annotation table by variables "table_id" and "column_id"

Update 2 Aug 2024
- adopted a variation on the ESS-DIVE approach to data dictionary and file level metadata to provide annotation/metadata info
- need to figure out directory structure
  - will quickly run out of space to work with large arrays...
- should store flmd & dd files in SWEDDIE directory; but also should keep these files with the data
- perhaps the data directory is stored in an external drive, and a copy of the metadata is written into the SWEDDIE directory each time data are compiled?
- I envision this compilation function creating a "meta" version of the database by compiling flmd & dd files
- the "meta" database is then queried to generate reports by reading in the actual data from the external source

# Analysis

Start with overall trends, i.e., simplify data to assess data on daily time step. Calculate mean, min, max, and sd for soil moisture and temperature. Treat plots as reps.

## Research Questions

1. Does heating influence soil moisture?
- use ANOVA type approach to compare heated/control plots
- could also look at regression of soil moisture and temperature, e.g., mixed modeling approach looking at treatment, site, etc., as potential variables affecting relationship
- OR, perhaps more simply, look at the difference between treatment and control soil moisture, and assess how different factors affect that difference

2. Does heating influence the depth profile of soil moisture?
- could use regression approach on control-heated soil moisture
- OR, ANOVA of different depths?
- Probably useful to treat depth as a continuous variable, but one potentially interacting with soil texture, vegetation (roots?), climate
- Might be useful to conduct the analysis within aridity/wetness index bins

3. How does site infrastructure/heating regime affect soil moisture trends?
- again, probably easiest to do in a regression context

5. Does aboveground vs. belowground heating affect soil moisture depth profiles?

6. Is there a heating level for which a given site exceeds a critical soil moisture deficit, e.g., wilting points/etc. [need to think about this more]

7. Does soil moisture affect the variability in heating in XYZ coordinates? For example, saturated soils could be expected to have more even heating; or, for sites with surficial heating, saturated soils may be slower to heat, e.g., at TRACE, KAEFS, etc.


## Hypotheses

1. Warmed plots will have lower soil moisture due to increased evaporation (and evapotranspiration?)
2. Differences between control and heated plot soil moisture will only be apparent in low moisture environments; is there a aridity/wetness index cut-off for this? 
3. Control-heated soil moisture differences will vary seasonally, greater during periods of increased soil moisture loss, i.e., summer


## Data
- soil temperature and moisture data
- depths
  - need to spline so that temperature and moisture are at same depths w/in sites
- climate:
  - *MAT*, *MAP*, *PET* (global), PET local (if available), ET, PAR (for calculating PET?)
- root biomass by depth?
- *soil texture*
- *soil organic matter*
- hydraulic conductivity (?)
- matric potential (?)


